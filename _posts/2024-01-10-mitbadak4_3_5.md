---
title: "밑바닥부터 시작하는 딥러닝 4.3~4.5"
categories: DeepLearning
tag: [DL, python]
header:
  overlay_image: assets/images/mitbadak.jpeg
  overlay_filter: 0.5
  teaser: assets/images/mitbadak.jpeg
---

<br/>

# 4.3 개선판 word2vec 학습

지금까지 기존 word2vec을 개선해보았다. 가장 먼저 Embedding 계층을 설명하고, 이어서 네거티브 샘플링 기법도 설명하였으며 이 두 가지를 실제로 구현하였다. 이 장에서는 이러한 개선을 신경망 구현에 적용해보며 PTB 데이터셋을 사용해 학습하고 더 실용적인 단어의 분산 표현을 얻어본다.



## 4.3.1 CBOW 모델 구현

앞 장의 단순한 SimpleCBOW 클래스를 개선한다.

개선점은 Embedding 계층과 Negative Sampling Loss 계층을 적용하는 것이다. 더 나아가 맥락의 윈도우 크기를 임의로 조절할 수 있도록 확장한다.

개선된 CBOW클래스의 모습은 다음과 같다.

``` python
import sys
sys.path.append('..')
import numpy as np
from common.layers import Embedding
from ch04.negative_sampling_layer import NegativeSamplingLoss

class CBOW :
  
  # 초기화 메서드
  def __init__(self, vocab_size, hidden_size, window_size, corpus) :
    # 가중치 초기화
    W_in = 0.01 * np.random.randn(V, H).astype('f')
    W_out = 0.01 * np.random.randn(V, H).astype('f')
    
    # 계층 생성
    self.in_layers = []
    for i in range(2 * window_size) :
      layer = Embedding(W_in) # Embedding 계층 사용
      self.in_layers.append(layer)
     self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)
    
    # 모든 가중치와 기울기를 배열에 모은다.
    layers = self.in_layers + [self.ns_loss]
    self.params, self.grads = [], []
    for layer in layers:
      self.params += layer.params
      self.grads += layer.grads
     
    # 인스턴스 변수에 단어의 분산 표현을 저장한다.
    self.word_vecs = W_in
    self.word_vecs2 = W_out
   
  def forward(self, contexts, target):
        h = 0
        for i, layer in enumerate(self.in_layers):
            h += layer.forward(contexts[:, i])
        h *= 1 / len(self.in_layers)  # average
        loss = self.ns_loss.forward(h, target)
        return loss
    
    def backward(self, dout=1):
        dout = self.ns_loss.backward(dout)
        dout *= 1 / len(self.in_layers)
        for layer in self.in_layers:
            layer.backward(dout)
        return None
```

* 초기화 메서드는 4개의 인수를 받는다. vocab_size 는 어휘 수, hidden_size 는 은닉층의 뉴런 수, corpus 는 단어 ID 목록이다. 그리고 맥락의 크기 (주변 단어 중 몇 개나 맥락으로 포함시킬지) 를 window_size로 지정한다. 즉 window_size가 2이면 타깃 단어의 좌우 2개씩, 총 4개 단어가 맥락이 된다.

**SimpleCBOW 클래스에서는 입력 측의 가중치와 출력 측의 가중치의 형상이 달라 출력 측의 가중치에서는 단어 벡터가 열 방향으로 배치되었다. 한편 CBOW 클래스의 출력 측 가중치는 입력 측 가중치와 같은 형상으로, 단어 벡터가 행 방향에 배치된다. 그 이유는 NegativeSamplingLoss 클래스에서 Embedding 계층을 사용하기 때문이다.** 

`forward()`,` backward()`는 각각 순전파, 역전파를 처리한다.

앞 장의 SimpleCBOW 클래스를 자연스럽게 확장한 것이다. 단 `forward(contexts, target)` 메서드가 인수로 받는 맥락과 타깃이 단어 ID라는 점이 다르다. (앞 장에서는 단어 ID를 원핫 벡터로 변환해서 사용하였다). 구체적인 예를 아래 그림으로 준비하였다.

![image4-19](assets/images/image4-19.png)

이 이미지의 오른쪽에 보이는 단어 ID의 배열이 contexts와 target의 예이다. 보다시피 contexts는 2차원 배열이고 target은 1차원 배열이다. 이러한 데이터가 `forward(contexts, target)` 에 입력되는 것이다. 이상이 CBOW 클래스의 설명이다. 

## 4.3.2 CBOW 모델 학습 코드

마지막으로 CBOW 모델의 학습을 구현한다. 여기에서는 단순히 신경망 학습을 수행할 뿐이다. 코드부터 살펴보자.

``` python
import sys
sys.path.append('..')
import numpy as np
from common import config
# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).
# ===============================================
# config.GPU = True
# ===============================================
import pickle
from common.trainer import Trainer
from common.optimizer import Adam
from cbow import CBOW
from skip_gram import SkipGram
from common.util import create_contexts_target, to_cpu, to_gpu
from dataset import ptb


# 하이퍼파라미터 설정
window_size = 5
hidden_size = 100
batch_size = 100
max_epoch = 10

# 데이터 읽기
corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)

contexts, target = create_contexts_target(corpus, window_size)
if config.GPU:
    contexts, target = to_gpu(contexts), to_gpu(target)

# 모델 등 생성
model = CBOW(vocab_size, hidden_size, window_size, corpus)
# model = SkipGram(vocab_size, hidden_size, window_size, corpus)
optimizer = Adam()
trainer = Trainer(model, optimizer)

# 학습 시작
trainer.fit(contexts, target, max_epoch, batch_size)
trainer.plot()

# 나중에 사용할 수 있도록 필요한 데이터 저장
word_vecs = model.word_vecs
if config.GPU:
    word_vecs = to_cpu(word_vecs)
params = {}
params['word_vecs'] = word_vecs.astype(np.float16)
params['word_to_id'] = word_to_id
params['id_to_word'] = id_to_word
pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'
with open(pkl_file, 'wb') as f:
    pickle.dump(params, f, -1)
```

* 해당 CBOW 모델은 윈도우 크기를 5로, 은닉층의 뉴런 수를 100개로 설정하였다. 사용하는 말뭉치에 따라 다르지만 윈도우 크기는 2~10개, 은닉층의 뉴런 수 (단어의 분산 표현의 차원 수)는 50~500개 정도면 좋은 결과를 얻을 수 있다. 
* 하지만 이번에 다루는 PTB는 지금까지의 말뭉치보다 월등히 커서 학습 시간이 상당히 오래 걸린다. 그래서 GPU를 사용할 수 있도록 작성하였다. 



## 4.3.3 CBOW 모델 평가

``` python
import sys
sys.path.append('..')
from common.util import most_similar, analogy
import pickle


pkl_file = 'cbow_params.pkl'
# pkl_file = 'skipgram_params.pkl'

with open(pkl_file, 'rb') as f:
    params = pickle.load(f)
    word_vecs = params['word_vecs']
    word_to_id = params['word_to_id']
    id_to_word = params['id_to_word']

# 가장 비슷한(most similar) 단어 뽑기
querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)

# 유추(analogy) 작업
print('-'*50)
analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)
analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)
analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)
analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)
```

위의 코드에서는 2장에서 구현한 `most_similar()` 메서드를 이용해 단어 몇 개에 대해 거리가 가장 가까운 단어들을 뽑이본다.

word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 가까이 모을 뿐 아니라 더 복잡한 패턴을 파악하는 것으로 알려져 있다. 대표적인 예가 "king - man + woman = queen" 으로 유명한 유추 문제(비유 문제) 이다. 더 정확하게 말하면 word2vec의 단어의 분산 표현을 사용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있다는 뜻이다.

실제로 유추 문제를 풀기 위해서 아래 이미지와 같이 단어 벡터 공간에서 "man -> woman" 벡터와 "king -> ?" 벡터가 가능한 한 가까워지는 단어를 찾는다.

![image4-20](assets/images/image4-20.png)

단어 "man"의 분산 표현(단어 벡터)을 "vec('man')" 이라고 표현해본다. 그러면 위의 이미지에서 얻고 싶은 관계를 수식으로 나타내면 "vec('king') + vec('woman') - vec('man') = vec(?)" 라는 벡터에 가장 가까운 단어 벡터를 구하는 일이 된다. 이 로직을 구현한 함수는 common/util.py 파일의 analogy() 이다. 이 함수를 사용하면 지금과 같은 유추 문제를 analogy('man', 'king', 'woman', word_to_id, id_to_word, word_vecs, top=5) 라는 한 줄로 처리할 수 있다. 

이처럼 word2vec으로 얻은 단어의 분산 표현을 사용하면 벡터의 덧셈과 뺄셈으로 유추 문제를 풀 수 있습니다. 단어의 단순한 의미뿐 아니라 문법적인 패턴도 파악할 수 있는 것이다. 그 밖에도 "good"과 "best" 사이에는 "better"가 존재한다고 하는 관계성 등 word2vec에서 얻은 단어의 분산 표현에는 흥미로운 결과가 얼마든지 발견되고 있다.



## 4.4.1 word2vec을 사용한 애플리케이션의 예

word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 찾는 용도로 이용할 수 있다. 그러나단어 분산 표현의 장점은 여기서 끝이 아니다. 자연어 처리 분야에서 단어의 분산 표현이 중요한 이유는 전이 학습(transfer learning)에 있다. 전이 학습은 한 분야에서 배운 지식을 다른 분야에도 적용하는 기법이다.

자연어 문제를 풀 때 word2vec의 단어 분산 표현을 처음부터 학습하는 일은 거의 없다. 하지만 그 대신 먼저 큰 말뭉치로 학습을 끝난 후 그 분산 표현을 각자의 작업에 이용하는 방식이다.

텍스트 분류, 문서 클러스터링, 품사 태그 달기, 감정 분석 등 자연어 처리 작업이라면 가장 먼저 단어를 벡터로 변환하는 작업을 해야 하는데, 이 때 학습을 미리 끝낸 단어의 분산 표현을 이용할 수 있다. 그리고 이 학습된 분산 표현이 방금 언급한 자연어 처리 작업 대부분에 좋은 결과를 가져다 준다.

단어의  분산 표현은 단어를 고정 길이 벡터로 변환해준다는 장점 또한 있다. 게다가 문장도 단어의 분산 표현을 사용하여 고정 길이 벡터로 변환할 수 있다. 문장을 고정 길이 벡터로 변환하는 방법은 활발하게 연구되고 있는데 가장 간단한 방법은 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구하는 것이다. 이를 bag-of-words라 하여 단어의 순서를 고려하지 않는 모델이다. 또한 5장에서 설명하는 순환 신경망 (RNN)을 사용하면 한층 세련된 방법으로 (word2vec의 단어의 분산 표현을 이용하면서) 문장을 고정 길이 벡터로 변환할 수 있다.

단어나 문장을 고정 길이 벡터로 변환할 수 있다는 점은 매우 중요하다. 자연어를 벡터로 변환할 수 있다면 일반적인 머신러닝 기법(신경망 또는 SVM 등)을 적용할 수 있기 때문이다.

이를 그림으로 나타내면 다음과 같이 된다.

![image4-21](assets/images/image4-21.png)

위의 이미지에서 보듯 자연어로 쓰여진 질문을 고정 길이 벡터로 변환할 수 있다면, 그 벡터를 다른 머신러닝 시스템의 입력으로 사용할 수 있다. 자연어를 벡터로 변환함으로써 일반적인 머신러닝 시스템의 틀에서 원하는 답을 출력하는 것(그리고 학습하는 것)이 가능해진다.



## 4.4.2 단어 벡터 평가 방법

word2vec을 통해 단어의 분산 표현을 얻을 수 있었다. 그렇다면 그 분산표현이 좋은지는 어떻게 평가하는지 알아보도록 한다.

